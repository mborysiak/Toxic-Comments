{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Overview\n",
    "This notebook generates non-neural network predictions using term frequency-inverse document frequency (TF-IDF) transformation of both word and character-level data. The resulting features are then modeled using Naive Bayes (NB), whose results are fed as features into a Logistic Regression algorithm, as shown by Wang and Manning using NB and Support Vector Machines (SVM): https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf\n",
    "\n",
    "This model generated a leaderboard score of 0.9800 on its own, which is approximately top 60th percentile. However, it blended very well in an ensemble with the various RNN predictions due to its completely different approach.\n",
    "\n",
    "This portion of the project was trained on my local machine, so the data loading and handling will be more traditional compared to the handling of data using Google's Datalab and Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from my_plot import PrettyPlot\n",
    "PrettyPlot(plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "There are six binary classes that need to be classified, shown in the list_classes list below. The training and testing comments are simply a 1-d vector consisting of words / sentences / paragraphs of varying lenghts. Depending on the words and messaging of the text, multiple categories can occur, i.e. a comment can be toxic, severe_toxic, and obscene at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = train['comment_text']\n",
    "test_comments = test['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Comments\n",
    "The code below using TF-IDF to create 1–3 gram word tokens, as well as 2–6 gram character tokens. The word and character tokens are created separately and then concantenated together to create the final feature set. 20000 word features and 40000 character features were kept for modeling. English stop words and unicode accents were removed during the transformation. Also the fitting process was performed on both the training and testing datasets, which could potentially lead to overfitting due to data leakage, but was necessary to improve performance as these sets were drawn from slightly different distributions due to changes in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(transformer, train, test):\n",
    "    \n",
    "    from scipy.sparse import hstack\n",
    "    transformer.fit(list(train) + list(test))\n",
    "    \n",
    "    train_features = transformer.transform(train)\n",
    "    test_features = transformer.transform(test)\n",
    "    \n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                  strip_accents='unicode',\n",
    "                                  analyzer='word',\n",
    "                                  token_pattern=r'\\w{1,}',\n",
    "                                  stop_words='english',\n",
    "                                  ngram_range=(1, 3),\n",
    "                                  max_features=20000)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                  strip_accents='unicode',\n",
    "                                  analyzer='char',\n",
    "                                  stop_words='english',\n",
    "                                  ngram_range=(2, 6),\n",
    "                                  max_features=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_features, test_word_features = vectorizer(word_vectorizer, train_comments, test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_features, test_char_features = vectorizer(char_vectorizer, train_comments, test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train = hstack([train_word_features, train_char_features], format='csr')\n",
    "X_test = hstack([test_word_features, test_char_features], format='csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model Functions\n",
    "The four functions below provide the Bayesian-Logistic Regression models, as well as cross-validation and train/predict code. The first function is calculates Bayesian probability of each category between a positive or negative occurence. This probability is then multiplied by the standard features and fed into the Logistic Regression model for final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Bayesian-Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates p(X | y = 1 or 0)\n",
    "def pr(y_i, y):\n",
    "    p = X_train[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling on Bayesian outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_mdl(estimator, y):\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    X_train_nb = X_train.multiply(r)\n",
    "    \n",
    "    return estimator.fit(X_train_nb, y), r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Cross-Validation and Train/Predict Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation on Bayesian Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cross_val(estimator, y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    X_train_nb = X_train.multiply(r)\n",
    "    results = cross_val_score(estimator, X=X_train_nb, y=y, cv=3, scoring='roc_auc')\n",
    "    \n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Create Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(estimator):\n",
    "    results = np.zeros((len(test), len(list_classes)))\n",
    "\n",
    "    for i, label in enumerate(list_classes):\n",
    "        print('fit', label)\n",
    "        estimator.fit(X_train, y_train[:,i])\n",
    "        try:\n",
    "            results[:,i] = estimator.predict_proba(X_test)[:,1]\n",
    "        except:\n",
    "            results[:,i] = estimator.predict_proba(X_test)[0][:,1]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Models\n",
    "Below, the cross-validation is performed for each separate class in the output. The output scores are averaged and reported below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "scores = {}\n",
    "\n",
    "lr = LogisticRegression(solver='sag')\n",
    "\n",
    "for i, j in enumerate(list_classes):\n",
    "    print('fit', j)\n",
    "    results = cross_val(lr, train[j])\n",
    "    scores[j] = np.mean(results)\n",
    "    \n",
    "mean_scores = round(np.mean(list(scores.values())), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9812"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Full Model and Creating Predictions\n",
    "Finally, the model is trained on the entire training dataset and the test set is predicted. The results are converted to a .csv file for submission to the Kaggle website below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "lr_results = train_and_predict(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test.id\n",
    "\n",
    "def create_submission(filename, ids, results):\n",
    "    id_series = pd.Series(ids, name='id')\n",
    "    results_df = pd.DataFrame(results, columns=list_classes)\n",
    "    combined = pd.concat([id_series, results_df], axis=1)\n",
    "    combined.to_csv(filename,index=False)\n",
    "    \n",
    "    return combined\n",
    "    \n",
    "submission = create_submission('../submissions/submission_45.csv', test_id, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
